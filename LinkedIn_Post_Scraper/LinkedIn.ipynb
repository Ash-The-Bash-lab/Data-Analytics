{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a2eac6-2987-4381-a39d-65dbcac49068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_github: https://github.com/StanislavLukashevich/LinkedIn-Python-Content-Analytics/blob/master/LinkedIn_WebScraping_Content_Performance_Analytics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c68c635b-8bd3-4882-9b4e-db4f6b5fcc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import getpass\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4eb4a1-c044-4f56-b2b0-d27d26626b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get User Informatio\n",
    "print(\"Enter Username\")\n",
    "username_cred =  str(input())\n",
    "print(\"Enter Password\")\n",
    "password_cred = getpass.getpass(\"Enter Password: \")\n",
    "print(\"How many posts are you interested in?\")\n",
    "post_count = int(input())\n",
    "\n",
    "print(\"********Details********\")\n",
    "print(f\"Username: {username_cred}\")\n",
    "print(\"Password:\", \"*\" * len(password_cred))\n",
    "print(f\"Count: {post_count}\")\n",
    "\n",
    "#create a browser-specific (Google Chrome) web navigation simulator\n",
    "#download from https://googlechromelabs.github.io/chrome-for-testing/ and download chromedrove.exe that matches your chrome version\n",
    "#to check your chrome verison open chrome -> settings -> about chrome\n",
    "#save this driver in the python env directory\n",
    "browser = webdriver.Chrome(\"C://Users//ashis//OneDrive//Desktop//MG Sustainable//02. HeatWise//WebScrape_LinkedIn//chromedriver.exe.\")\n",
    "\n",
    "#opens the LinkedIn login page and login under a specified account:\n",
    "browser.get('https://www.linkedin.com/login')\n",
    "\n",
    "#enter the specified information to login to LinkedIn:\n",
    "elementID = browser.find_element_by_id('username')\n",
    "elementID.send_keys(username_cred)\n",
    "time.sleep(5)\n",
    "\n",
    "elementID = browser.find_element_by_id('password')\n",
    "elementID.send_keys(password_cred)\n",
    "time.sleep(5)\n",
    "\n",
    "elementID.submit()\n",
    "\n",
    "#direct to HEATWISE's linkedin page\n",
    "activity_link = \"https://www.linkedin.com/company/heatwise-horizon-europe-project-2023/posts/?feedView=all\" \n",
    "time.sleep(2)\n",
    "browser.get(activity_link)\n",
    "\n",
    "#calculate number of scrolls depending on the input\n",
    "number_of_scrolls = -(-post_count // 5)  # 5 is LinkedIn's number of posts per scroll based on screen size\n",
    "\n",
    "#we need a loop because we have a particular number of scrolls...\n",
    "likes = []\n",
    "comments = []\n",
    "\n",
    "SCROLL_PAUSE_TIME = 5\n",
    "\n",
    "# Get scroll height\n",
    "last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "for scroll in range(number_of_scrolls) : \n",
    "    #scroll down to bottom\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    #wait to load page\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    #calculate new scroll height and compare with last scroll height\n",
    "    new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height    \n",
    "\n",
    "#query the contents (returns service reponse object with web contents, url headers, status and other):\n",
    "src = browser.page_source\n",
    "#beautiful soup instance:\n",
    "soup = BeautifulSoup(src, features=\"lxml\")   #lxml\n",
    "    \n",
    "#find LIKES on LinkedIn\n",
    "#look for \"span\" tags that have the specific following attribute (click 'inspect' on the L-in page)\n",
    "#need to convert the list of bs4 tags into strings and then extract \n",
    "#find these specific tags (\"<stuff>\") in the soup contents:\n",
    "\n",
    "likes_bs4tags = soup.find_all(\"span\", attrs = {\"class\" : \"social-details-social-counts__reactions-count\"})\n",
    "#converts a list of 1 string to int, appends to likes list\n",
    "for tag in likes_bs4tags:\n",
    "    strtag = str(tag)\n",
    "    #the first argument in findall (below) is a regular expression (accounts for commas in the number)\n",
    "    list_of_matches = re.findall('[,0-9]+',strtag)\n",
    "    #converts the last element (string) in the list to int, appends to likes list\n",
    "    last_string = list_of_matches.pop()\n",
    "    without_comma = last_string.replace(',','')\n",
    "    likes_int = int(without_comma)\n",
    "    likes.append(likes_int)\n",
    "\n",
    "comments_bs4tags = soup.find_all(\"li\", class_=\"social-details-social-counts__item social-details-social-counts__comments social-details-social-counts__item--right-aligned\")\n",
    "\n",
    "for tag in comments_bs4tags:\n",
    "    #find the button tag within the list item\n",
    "    button_tag = tag.find(\"button\")\n",
    "    \n",
    "    #check if the button tag and aria-label exist\n",
    "    if button_tag and 'aria-label' in button_tag.attrs:\n",
    "        aria_label = button_tag['aria-label']\n",
    "        print(aria_label)\n",
    "        \n",
    "        #extract the number from the aria-label\n",
    "        comments_count = int(re.search(r'\\d+', aria_label).group())\n",
    "    else:\n",
    "        #default to 0 if not found\n",
    "        comments_count = 0\n",
    "    \n",
    "    #a the count to the list\n",
    "    comments.append(comments_count)\n",
    "\n",
    "#calculate total likes and comments\n",
    "total_likes = sum(likes)\n",
    "total_comms = sum(comments)\n",
    "\n",
    "new_row = {\n",
    "    'no_likes': total_likes,\n",
    "    'no_comments': total_comms,\n",
    "    'date': datetime.today().date()\n",
    "}\n",
    "\n",
    "#create a DataFrame with the new row\n",
    "new_df = pd.DataFrame([new_row])\n",
    "\n",
    "df = pd.read_excel(\"raw_data.xlsx\")\n",
    "\n",
    "df = pd.concat([df, new_df], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42bcad81-1006-4bb5-9a05-9057ad434554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as CSV or xlsx\n",
    "df.to_excel('raw_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ebe0b-43fe-4d75-8a93-e374beedfb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
